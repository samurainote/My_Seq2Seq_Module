{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Dense, Input\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_UNITS = 100\n",
    "DEFAULT_BATCH_SIZE = 64\n",
    "VERBOSE = 1\n",
    "DEFAULT_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_summarizer(document,\n",
    "                      num_input_tokens,\n",
    "                      max_input_seq_length,\n",
    "                      num_target_tokens,\n",
    "                      max_target_seq_length,\n",
    "                      input_word2idx,\n",
    "                      input_idx2word,\n",
    "                      target_word2idx,\n",
    "                      target_idx2word):\n",
    "    \n",
    "    # Encoder\n",
    "    encoder_input = Input(shape=(None,), name=\"encoder_input\")\n",
    "    encoder_input2embedding = Embedding(input_dim=imput_dict_size, output_dim=HIDDEN_UNITS, input_length=max_input_seq_length, name=\"encoder_input2embedding\")\n",
    "    embedding2lstm = LSTM(units=HIDDEN_UNITS, return_state=True, return_sequences=True, dropout=0.3, name=\"embedding2lstm\")    \n",
    "    # \n",
    "    encoder_outputs, encoder_state_h, encoder_state_c = embedding2lstm(encoder_input2embedding(encoder_input))\n",
    "    encoder_states = [encoder_state_h, encoder_state_c] \n",
    "\n",
    "    # Decoder\n",
    "    decoder_input = Input(shape=(None, self.num_target_tokens), name=\"decoder_input\")\n",
    "    decoder_input2lstm = LSTM(units=HIDDEN_UNITS, return_state=True, return_sequences=True, dropout=0.3, name=\"decoder_input2lstm\")\n",
    "    # \n",
    "    decoder_outputs, decoder_state_h, decoder_state_c = decoder_input2lstm(decoder_input, initial_state=encoder_states)\n",
    "\n",
    "    lstm2softmax = Dense(units=self.num_target_tokens, activation=\"softmax\", name=\"lstm2softmax\")\n",
    "    softmax2output = lstm2softmax(decoder_outputs)\n",
    "\n",
    "    # Encoder-Decoder Modelling\n",
    "    model = Model(inputs=[encoder_input, decoder_input], outputs=[softmax2output])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    model = model\n",
    "\n",
    "    # Encoder Modelling\n",
    "    self.encoder_model = Model(encoder_input, encoder_states)\n",
    "\n",
    "    # Decoder Modelling\n",
    "    ddecoder_state_inputs = [Input(shape=(HIDDEN_UNITS,)), Input(shape=(HIDDEN_UNITS,))]\n",
    "    decoder_outputs, state_h, state_c = decoder_input2lstm(decoder_input, initial_state=decoder_state_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = lstm2softmax(decoder_outputs)\n",
    "    self.decoder_model = Model([decoder_input] + decoder_state_inputs, [decoder_outputs] + decoder_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-11-86142d01c521>, line 53)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-86142d01c521>\"\u001b[0;36m, line \u001b[0;32m53\u001b[0m\n\u001b[0;31m    if os.path.exists(weight_file_path):\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class seq2seq_document_summarizer(object):\n",
    "    \n",
    "\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.num_input_tokens = config['num_input_tokens']\n",
    "        self.max_input_seq_length = config['max_input_seq_length']\n",
    "        self.num_target_tokens = config['num_target_tokens']\n",
    "        self.max_target_seq_length = config['max_target_seq_length']\n",
    "        self.input_word2idx = config['input_word2idx']\n",
    "        self.input_idx2word = config['input_idx2word']\n",
    "        self.target_word2idx = config['target_word2idx']\n",
    "        self.target_idx2word = config['target_idx2word']\n",
    "        self.config = config\n",
    "\n",
    "        self.version = 0\n",
    "        if 'version' in config:\n",
    "            self.version = config['version']\n",
    "\n",
    "        # Encoder\n",
    "        encoder_input = Input(shape=(None,), name=\"encoder_input\")\n",
    "        encoder_input2embedding = Embedding(input_dim=imput_dict_size, output_dim=HIDDEN_UNITS, input_length=self.max_input_seq_length, name=\"encoder_input2embedding\")\n",
    "        embedding2lstm = LSTM(units=HIDDEN_UNITS, return_state=True, return_sequences=True, dropout=0.3, name=\"embedding2lstm\")\n",
    "        \n",
    "        encoder_outputs, encoder_state_h, encoder_state_c = embedding2lstm(encoder_input2embedding(encoder_input))\n",
    "        encoder_states = [encoder_state_h, encoder_state_c] \n",
    "        \n",
    "        # Decoder\n",
    "        decoder_input = Input(shape=(None, self.num_target_tokens), name=\"decoder_input\")\n",
    "        decoder_input2lstm = LSTM(units=HIDDEN_UNITS, return_state=True, return_sequences=True, dropout=0.3, name=\"decoder_input2lstm\")\n",
    "        decoder_outputs, decoder_state_h, decoder_state_c = decoder_input2lstm(decoder_input, initial_state=encoder_states)\n",
    "        \n",
    "        lstm2softmax = Dense(units=self.num_target_tokens, activation=\"softmax\", name=\"lstm2softmax\")\n",
    "        softmax2output = lstm2softmax(decoder_outputs)\n",
    "\n",
    "        # Encoder-Decoder Modelling\n",
    "        model = Model(inputs=[encoder_input, decoder_input], outputs=[softmax2output])\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        self.model = model\n",
    "        \n",
    "        # Encoder Modelling\n",
    "        self.encoder_model = Model(encoder_input, encoder_states)\n",
    "        \n",
    "        # Decoder Modelling\n",
    "        ddecoder_state_inputs = [Input(shape=(HIDDEN_UNITS,)), Input(shape=(HIDDEN_UNITS,))]\n",
    "        decoder_outputs, state_h, state_c = decoder_input2lstm(decoder_input, initial_state=decoder_state_inputs)\n",
    "        decoder_states = [state_h, state_c]\n",
    "        decoder_outputs = lstm2softmax(decoder_outputs)\n",
    "        self.decoder_model = Model([decoder_input] + decoder_state_inputs, [decoder_outputs] + decoder_states)\n",
    "        \n",
    "        # \n",
    "        def load_weights(self, weight_file_path):\n",
    "        if os.path.exists(weight_file_path):\n",
    "            self.model.load_weights(weight_file_path)\n",
    "            \n",
    "            \n",
    "        def transform_input_text(self, texts):\n",
    "            temp = []\n",
    "            for line in texts:\n",
    "                x = []\n",
    "                for word in line.lower().split(' '):\n",
    "                    wid = 1\n",
    "                    if word in self.input_word2idx:\n",
    "                        wid = self.input_word2idx[word]\n",
    "                    x.append(wid)\n",
    "                    if len(x) >= self.max_input_seq_length:\n",
    "                        break\n",
    "                temp.append(x)\n",
    "            temp = pad_sequences(temp, maxlen=self.max_input_seq_length)\n",
    "\n",
    "            print(temp.shape)\n",
    "            return temp\n",
    "    \n",
    "    \n",
    "        def transform_target_encoding(self, texts):\n",
    "            temp = []\n",
    "            for line in texts:\n",
    "                x = []\n",
    "                line2 = 'START ' + line.lower() + ' END'\n",
    "                for word in line2.split(' '):\n",
    "                    x.append(word)\n",
    "                    if len(x) >= self.max_target_seq_length:\n",
    "                        break\n",
    "                temp.append(x)\n",
    "\n",
    "            temp = np.array(temp)\n",
    "            print(temp.shape)\n",
    "            return temp\n",
    "        \n",
    "        def generate_batch(self, x_samples, y_samples, batch_size):\n",
    "        num_batches = len(x_samples) // batch_size\n",
    "        while True:\n",
    "            for batchIdx in range(0, num_batches):\n",
    "                start = batchIdx * batch_size\n",
    "                end = (batchIdx + 1) * batch_size\n",
    "                encoder_input_data_batch = pad_sequences(x_samples[start:end], self.max_input_seq_length)\n",
    "                decoder_target_data_batch = np.zeros(shape=(batch_size, self.max_target_seq_length, self.num_target_tokens))\n",
    "                decoder_input_data_batch = np.zeros(shape=(batch_size, self.max_target_seq_length, self.num_target_tokens))\n",
    "                for lineIdx, target_words in enumerate(y_samples[start:end]):\n",
    "                    for idx, w in enumerate(target_words):\n",
    "                        w2idx = 0  # default [UNK]\n",
    "                        if w in self.target_word2idx:\n",
    "                            w2idx = self.target_word2idx[w]\n",
    "                        if w2idx != 0:\n",
    "                            decoder_input_data_batch[lineIdx, idx, w2idx] = 1\n",
    "                            if idx > 0:\n",
    "                                decoder_target_data_batch[lineIdx, idx - 1, w2idx] = 1\n",
    "                yield [encoder_input_data_batch, decoder_input_data_batch], decoder_target_data_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def get_weight_file_path(model_dir_path):\n",
    "        return model_dir_path + '/' + Seq2SeqGloVeSummarizer.model_name + '-weights.h5'\n",
    "\n",
    "    @staticmethod\n",
    "    def get_config_file_path(model_dir_path):\n",
    "        return model_dir_path + '/' + Seq2SeqGloVeSummarizer.model_name + '-config.npy'\n",
    "\n",
    "    @staticmethod\n",
    "    def get_architecture_file_path(model_dir_path):\n",
    "        return model_dir_path + '/' + Seq2SeqGloVeSummarizer.model_name + '-architecture.json'\n",
    "\n",
    "    def fit(self, Xtrain, Ytrain, Xtest, Ytest, epochs=None, batch_size=None, model_dir_path=None):\n",
    "        if epochs is None:\n",
    "            epochs = DEFAULT_EPOCHS\n",
    "        if model_dir_path is None:\n",
    "            model_dir_path = './models'\n",
    "        if batch_size is None:\n",
    "            batch_size = DEFAULT_BATCH_SIZE\n",
    "\n",
    "        self.version += 1\n",
    "        self.config['version'] = self.version\n",
    "        config_file_path = Seq2SeqGloVeSummarizer.get_config_file_path(model_dir_path)\n",
    "        weight_file_path = Seq2SeqGloVeSummarizer.get_weight_file_path(model_dir_path)\n",
    "        checkpoint = ModelCheckpoint(weight_file_path)\n",
    "        np.save(config_file_path, self.config)\n",
    "        architecture_file_path = Seq2SeqGloVeSummarizer.get_architecture_file_path(model_dir_path)\n",
    "        open(architecture_file_path, 'w').write(self.model.to_json())\n",
    "\n",
    "        Ytrain = self.transform_target_encoding(Ytrain)\n",
    "        Ytest = self.transform_target_encoding(Ytest)\n",
    "\n",
    "        Xtrain = self.transform_input_text(Xtrain)\n",
    "        Xtest = self.transform_input_text(Xtest)\n",
    "\n",
    "        train_gen = self.generate_batch(Xtrain, Ytrain, batch_size)\n",
    "        test_gen = self.generate_batch(Xtest, Ytest, batch_size)\n",
    "\n",
    "        train_num_batches = len(Xtrain) // batch_size\n",
    "        test_num_batches = len(Xtest) // batch_size\n",
    "\n",
    "        history = self.model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
    "                                           epochs=epochs,\n",
    "                                           verbose=VERBOSE, validation_data=test_gen, validation_steps=test_num_batches,\n",
    "                                           callbacks=[checkpoint])\n",
    "        self.model.save_weights(weight_file_path)\n",
    "        return history\n",
    "\n",
    "    def summarize(self, input_text):\n",
    "        input_seq = np.zeros(shape=(1, self.max_input_seq_length, GLOVE_EMBEDDING_SIZE))\n",
    "        for idx, word in enumerate(input_text.lower().split(' ')):\n",
    "            if idx >= self.max_input_seq_length:\n",
    "                break\n",
    "            emb = self.unknown_emb  # default [UNK]\n",
    "            if word in self.word2em:\n",
    "                emb = self.word2em[word]\n",
    "            input_seq[0, idx, :] = emb\n",
    "        states_value = self.encoder_model.predict(input_seq)\n",
    "        target_seq = np.zeros((1, 1, self.num_target_tokens))\n",
    "        target_seq[0, 0, self.target_word2idx['START']] = 1\n",
    "        target_text = ''\n",
    "        target_text_len = 0\n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            output_tokens, h, c = self.decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "            sample_token_idx = np.argmax(output_tokens[0, -1, :])\n",
    "            sample_word = self.target_idx2word[sample_token_idx]\n",
    "            target_text_len += 1\n",
    "\n",
    "            if sample_word != 'START' and sample_word != 'END':\n",
    "                target_text += ' ' + sample_word\n",
    "\n",
    "            if sample_word == 'END' or target_text_len >= self.max_target_seq_length:\n",
    "                terminated = True\n",
    "\n",
    "            target_seq = np.zeros((1, 1, self.num_target_tokens))\n",
    "            target_seq[0, 0, sample_token_idx] = 1\n",
    "\n",
    "            states_value = [h, c]\n",
    "        return target_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
